{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generic Agent Evaluation\n",
    "\n",
    "Good evaluation is key for quickly iterating on your agent's prompts and tools. Here we provide an example of how to use the ReactEvalChain to evaluate your agent."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "Let's start by defining our agent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import Wikipedia\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.agents import initialize_agent, Tool\n",
    "from langchain.agents import AgentType\n",
    "from langchain.agents.react.base import DocstoreExplorer\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain import LLMMathChain\n",
    "from langchain.llms import OpenAI\n",
    "\n",
    "from langchain import SerpAPIWrapper\n",
    "\n",
    "docstore = DocstoreExplorer(Wikipedia())\n",
    "\n",
    "math_llm = OpenAI(temperature=0)\n",
    "\n",
    "llm_math_chain = LLMMathChain(llm=math_llm, verbose=True)\n",
    "\n",
    "search = SerpAPIWrapper()\n",
    "\n",
    "tools = [\n",
    "    Tool(\n",
    "        name=\"Search\",\n",
    "        func=docstore.search,\n",
    "        description=\"useful for when you need to ask with search\",\n",
    "    ),\n",
    "    Tool(\n",
    "        name=\"Lookup\",\n",
    "        func=docstore.lookup,\n",
    "        description=\"useful for when you need to ask with lookup\",\n",
    "    ),\n",
    "    Tool(\n",
    "        name=\"Calculator\",\n",
    "        func=llm_math_chain.run,\n",
    "        description=\"useful for doing calculations\",\n",
    "    ),\n",
    "    Tool(\n",
    "        name=\"Search the Web (SerpAPI)\",\n",
    "        func=search.run,\n",
    "        description=\"useful for when you need to answer questions about current events\",\n",
    "    ),\n",
    "]\n",
    "\n",
    "memory = ConversationBufferMemory(\n",
    "    memory_key=\"chat_history\", return_messages=True, output_key=\"output\"\n",
    ")\n",
    "\n",
    "llm = ChatOpenAI(temperature=0, model_name=\"gpt-3.5-turbo\")\n",
    "\n",
    "agent = initialize_agent(\n",
    "    tools,\n",
    "    llm,\n",
    "    agent=AgentType.CHAT_CONVERSATIONAL_REACT_DESCRIPTION,\n",
    "    verbose=True,\n",
    "    memory=memory,\n",
    "    return_intermediate_steps=True, # This is needed for the evaluation later\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing the Agent\n",
    "\n",
    "Now let's try our agent out on some example queries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m{\n",
      "    \"action\": \"Calculator\",\n",
      "    \"action_input\": \"37,000,000 cubic feet / 2.5 cubic inches per ping pong ball\"\n",
      "}\u001b[0m\n",
      "\n",
      "\u001b[1m> Entering new LLMMathChain chain...\u001b[0m\n",
      "37,000,000 cubic feet / 2.5 cubic inches per ping pong ball\u001b[32;1m\u001b[1;3m\n",
      "```python\n",
      "print(37000000 / (2.5 * (1/1728)))\n",
      "```\n",
      "\u001b[0m\n",
      "Answer: \u001b[33;1m\u001b[1;3m25574400000.0\n",
      "\u001b[0m\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "Observation: \u001b[38;5;200m\u001b[1;3mAnswer: 25574400000.0\n",
      "\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3m{\n",
      "    \"action\": \"Final Answer\",\n",
      "    \"action_input\": \"It would take approximately 25.6 billion ping pong balls to fill the Empire State Building.\"\n",
      "}\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "query_one = \"How many ping pong balls would it take to fill the entire Empire State Building?\"\n",
    "\n",
    "test_outputs_one = agent({\"input\": query_one}, return_only_outputs=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This looks good! Let's try it out on another query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m{\n",
      "    \"action\": \"Search\",\n",
      "    \"action_input\": \"length of the Eiffel Tower in miles\"\n",
      "}\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3mThe following are examples of orders of magnitude for different lengths.\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3m{\n",
      "    \"action\": \"Final Answer\",\n",
      "    \"action_input\": \"I apologize for the confusion in my previous response. Unfortunately, I was mistaken in suggesting that the Eiffel Tower could be laid end to end to cover the US from coast to coast. The Eiffel Tower is approximately 1,063 feet tall, which is much shorter than the width of the United States. To cover the US from coast to coast, you would need a much larger structure or object. I hope this helps clarify things for you.\"\n",
      "}\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "query_two = \"If you laid the Eiffel Tower end to end, how many would you need cover the US from coast to coast?\"\n",
    "\n",
    "test_outputs_two = agent({\"input\": query_two}, return_only_outputs=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This doesn't look so good. Let's try running some evaluation.\n",
    "\n",
    "## Evaluating the Agent\n",
    "\n",
    "Let's start by defining the ReactEvalChain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.evaluation.react import ReactEvalChain\n",
    "\n",
    "# Define chain\n",
    "react_eval_chain = ReactEvalChain.from_llm(\n",
    "    llm=ChatOpenAI(temperature=0, model_name=\"gpt-4\"), # Note: This must be a ChatOpenAI model\n",
    "    agent_tools=agent.tools,\n",
    "    return_reasoning=True,\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try evaluating the first query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score from 1 to 5:  3\n",
      "Reasoning:  First, let's evaluate the final answer. The final answer is helpful as it provides an approximate number of ping pong balls needed to fill the Empire State Building. However, the model did not provide any information on how it arrived at the 37,000,000 cubic feet figure for the Empire State Building's volume.\n",
      "\n",
      "Second, does the model use a logical sequence of tools to answer the question? The model only used the Calculator tool, but it should have used the Search or Lookup tool to find the volume of the Empire State Building and the volume of a ping pong ball.\n",
      "\n",
      "Third, does the AI language model use the tools in a helpful way? The model used the Calculator tool in a helpful way to perform the calculation, but it did not use the Search or Lookup tool to find the necessary information.\n",
      "\n",
      "Fourth, does the AI language model use too many steps to answer the question? The model used only one step, which is not too many, but it should have used additional steps to find the required information.\n",
      "\n",
      "Fifth, are the appropriate tools used to answer the question? The model used the Calculator tool appropriately but did not use the Search or Lookup tool to find the volume of the Empire State Building and the volume of a ping pong ball.\n",
      "\n",
      "Judgment: Given the helpful final answer but the lack of proper use of tools, we give the model a score of 3.\n"
     ]
    }
   ],
   "source": [
    "question, steps, answer = test_outputs_one[\"input\"], test_outputs_one[\"intermediate_steps\"], test_outputs_one[\"output\"]\n",
    "\n",
    "evaluation = react_eval_chain(\n",
    "    inputs={\"question\": question, \"answer\": answer, \"agent_trajectory\": react_eval_chain.get_agent_trajectory(steps)},\n",
    ")\n",
    "\n",
    "print(\"Score from 1 to 5: \", evaluation[\"score\"])\n",
    "print(\"Reasoning: \", evaluation[\"reasoning\"])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That seems about right. Let's try the second query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score from 1 to 5:  1\n",
      "Reasoning:  i. Is the final answer helpful?\n",
      "The final answer is not helpful because it does not provide the information requested by the user. The user asked how many Eiffel Towers laid end to end would be needed to cover the US from coast to coast, but the answer only states that the Eiffel Tower is much shorter than the width of the United States.\n",
      "\n",
      "ii. Does the AI language use a logical sequence of tools to answer the question?\n",
      "The AI language model used only one tool, which was the Search tool. However, it did not use the tool effectively to find the necessary information to answer the question.\n",
      "\n",
      "iii. Does the AI language model use the tools in a helpful way?\n",
      "The AI language model did not use the tools in a helpful way. The Search tool was used to find the length of the Eiffel Tower, but the output was not helpful, and the model did not attempt to find the width of the United States or use the Calculator tool to determine how many Eiffel Towers would be needed.\n",
      "\n",
      "iv. Does the AI language model use too many steps to answer the question?\n",
      "The AI language model used only one step, which was not enough to answer the question effectively.\n",
      "\n",
      "v. Are the appropriate tools used to answer the question?\n",
      "The appropriate tools were not used to answer the question. The model should have used the Search tool to find both the height of the Eiffel Tower and the width of the United States, and then used the Calculator tool to determine how many Eiffel Towers would be needed to cover the US from coast to coast.\n",
      "\n",
      "Given the above evaluation, the AI language model's performance is poor, and we give it a score of 1.\n"
     ]
    }
   ],
   "source": [
    "question, steps, answer = test_outputs_two[\"input\"], test_outputs_two[\"intermediate_steps\"], test_outputs_two[\"output\"]\n",
    "\n",
    "evaluation = react_eval_chain(\n",
    "    inputs={\"question\": question, \"answer\": answer, \"agent_trajectory\": react_eval_chain.get_agent_trajectory(steps)},\n",
    ")\n",
    "\n",
    "print(\"Score from 1 to 5: \", evaluation[\"score\"])\n",
    "print(\"Reasoning: \", evaluation[\"reasoning\"])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That also sounds about right. In conclusion, the ReactEvalChain allows us to use GPT-4 to score both our agent's outputs and tool use in addition to giving us the reasoning behind the evaluation."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langchain",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "06ba49dd587e86cdcfee66b9ffe769e1e94f0e368e54c2d6c866e38e33c0d9b1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
